

# --- START: core/file_processor.py ---
# core/file_processor.py ---
"""
Handles processing related to file content generated by the LLM.
Includes extracting structured data (e.g., JSON) from the LLM response
and writing updated file contents to the local disk.
"""

import logging
import json
import yaml # Required if YAML parsing is needed
import os
import re # For regex-based extraction
from typing import Dict, Optional, List # Use Dict, Optional, List

from .exceptions import ParsingError, FileProcessingError

logger: logging.Logger = logging.getLogger(__name__)

class FileProcessor:
    """
    Provides methods for parsing LLM output and saving generated file content.
    """
    def init(self: 'FileProcessor') -> None:
        """Initialises the FileProcessor."""
        logger.debug("FileProcessor initialised.")
        # TODO: Accept configuration if needed (e.g., default output format)

    def extractCodeBlock(self: 'FileProcessor', llmResponse: str, language: str = 'json') -> Optional[str]:
        """
        Extracts the content of the first fenced code block (e.g., ```json ... ```)
        from the LLM's response string.

        Args:
            llmResponse (str): The raw response string from the LLM.
            language (str): The language identifier of the code block (e.g., 'json', 'yaml', 'python').
                            Defaults to 'json'. If empty string, matches any ``` block.

        Returns:
            Optional[str]: The extracted content within the code block, or None if no
                        matching block is found.

        # TODO: Make the extraction more robust, handling potential variations in markdown formatting.
        """
        logger.debug(f"Attempting to extract '{language}' code block...")

        # Pattern explanation:
        # ``` optionally followed by the language identifier (case-insensitive)
        # \s*?       optional whitespace (non-greedy)
        # \n         a newline
        # (.*?)      capture group 1: the content (non-greedy)
        # \n         a newline
        # ```        closing fence
        # re.DOTALL makes '.' match newlines, re.IGNORECASE for language identifier
        pattern = rf"```{language}\s*?\n(.*?)\n```"
        match = re.search(pattern, llmResponse, re.DOTALL | re.IGNORECASE)

        if match:
            extractedContent: str = match.group(1).strip()
            logger.info(f"Successfully extracted '{language}' code block. Length: {len(extractedContent)}")
            return extractedContent
        else:
            # Fallback: Try finding generic block if specific language not found
            if language: # Only try generic if a specific language was requested and not found
                logger.warning(f"No specific '```{language}' block found. Trying generic '```...```' block.")
                pattern = r"```\s*?\n(.*?)\n```"
                match = re.search(pattern, llmResponse, re.DOTALL)
                if match:
                        extractedContent = match.group(1).strip()
                        logger.info(f"Successfully extracted generic code block. Length: {len(extractedContent)}")
                        # We assume this generic block IS the intended format (e.g., JSON)
                        return extractedContent

            logger.warning(f"Could not find a fenced code block matching '```{language}' or generic '```'.")
            return None

    def parseStructuredOutput(self: 'FileProcessor', structuredDataString: str, format: str = 'json') -> Dict[str, str]:
        """
        Parses the extracted structured data string (e.g., JSON, YAML) into a dictionary.
        Validates that the output is a dictionary mapping filenames (str) to content (str).

        Args:
            structuredDataString (str): The string content extracted from the code block.
            format (str): The expected format ('json' or 'yaml'). Defaults to 'json'.

        Returns:
            Dict[str, str]: A dictionary where keys are relative file paths and values
                            are the corresponding file contents.

        Raises:
            ParsingError: If the string is not valid for the specified format, or if
                        the parsed structure is not Dict[str, str].
            NotImplementedError: If an unsupported format is requested.
        """
        logger.info(f"Parsing structured output as '{format}'. Length: {len(structuredDataString)}")
        parsedData: Dict[str, str] = {} # Use Dict

        try:
            if format == 'json':
                parsedData = json.loads(structuredDataString)
            elif format == 'yaml':
                # # TODO: Ensure PyYAML is installed if using this
                parsedData = yaml.safe_load(structuredDataString)
            else:
                raise NotImplementedError(f"Parsing for format '{format}' is not implemented.")

            # --- Validation ---
            if not isinstance(parsedData, dict):
                errMsg = f"Parsed data is not a dictionary as expected. Found type: {type(parsedData).__name__}"
                logger.error(errMsg)
                raise ParsingError(errMsg)

            # Check key/value types
            for key, value in parsedData.items():
                if not isinstance(key, str):
                    errMsg = f"Invalid structure: Dictionary key '{key}' is not a string (type: {type(key).__name__})."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)
                if not isinstance(value, str):
                    errMsg = f"Invalid structure: Value for key '{key}' is not a string (type: {type(value).__name__})."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)
                # # TODO: Add validation for file paths? (e.g., check for invalid characters?)

            logger.info(f"Successfully parsed '{format}' data. Found {len(parsedData)} file entries.")
            return parsedData

        except json.JSONDecodeError as e:
            errMsg = f"Invalid JSON detected: {e}. Check the LLM response format."
            logger.error(errMsg, exc_info=True)
            raise ParsingError(errMsg) from e
        except yaml.YAMLError as e:
            errMsg = f"Invalid YAML detected: {e}. Check the LLM response format."
            logger.error(errMsg, exc_info=True)
            raise ParsingError(errMsg) from e
        except NotImplementedError as e:
            logger.error(str(e))
            raise e # Re-raise NotImplementedError
        except ParsingError as e: # Re-raise validation errors
            raise e
        except Exception as e: # Catch other unexpected errors
            errMsg = f"An unexpected error occurred during parsing: {e}"
            logger.error(errMsg, exc_info=True)
            raise ParsingError(errMsg) from e

    def saveFilesToDisk(self: 'FileProcessor', outputDir: str, fileData: Dict[str, str]) -> List[str]:
        """
        Saves the file contents from the parsed dictionary to the specified output directory.
        Creates necessary subdirectories and overwrites existing files.

        Args:
            outputDir (str): The base directory (e.g., the cloned repo path) where files should be saved.
            fileData (Dict[str, str]): The dictionary mapping relative file paths to their content.

        Returns:
            List[str]: A list of the relative paths of the files that were successfully saved.

        Raises:
            FileProcessingError: If there are errors creating directories or writing files
                                (e.g., permissions, disk space).
        """
        logger.info(f"Saving {len(fileData)} files to base directory: {outputDir}")
        savedFilesList: List[str] = [] # Use List

        if not os.path.isdir(outputDir):
            errMsg = f"Output directory '{outputDir}' does not exist or is not a directory."
            logger.error(errMsg)
            raise FileProcessingError(errMsg)

        for relativePath, content in fileData.items():
            # Clean/validate relative path? Prevent '..' etc.
            # os.path.abspath protects against some '..' tricks if outputDir is absolute
            # For safety, disallow '..' in relative paths explicitly
            if ".." in relativePath.split(os.path.sep):
                    errMsg = f"Invalid relative path contains '..': '{relativePath}'. Skipping."
                    logger.error(errMsg)
                    # Decide whether to raise error or just skip
                    # raise FileProcessingError(errMsg)
                    continue # Skip this file

            # Ensure path uses correct OS separator (though LLM likely uses '/')
            # os.path.join handles this if outputDir is clean.
            # If relativePath uses mixed separators, it might be an issue.
            # Normalise the relative path separators first?
            normalisedRelativePath = os.path.normpath(relativePath)
            if normalisedRelativePath.startswith(".."): # Double check after normpath
                    errMsg = f"Invalid relative path resolves outside base directory: '{relativePath}'. Skipping."
                    logger.error(errMsg)
                    continue

            fullPath: str = os.path.join(outputDir, normalisedRelativePath)
            logger.debug(f"Preparing to save file: {fullPath}")

            try:
                # Create parent directories if they don't exist
                fileDir: str = os.path.dirname(fullPath)
                if fileDir and not os.path.exists(fileDir):
                    logger.debug(f"Creating directory: {fileDir}")
                    os.makedirs(fileDir, exist_ok=True)

                # Write the file content (overwrite if exists)
                with open(fullPath, 'w', encoding='utf-8') as fileHandle:
                    fileHandle.write(content)

                savedFilesList.append(relativePath) # Log the original relative path
                logger.debug(f"Successfully wrote file: {fullPath}")

            except OSError as e:
                errMsg = f"OS error writing file '{fullPath}': {e}"
                logger.error(errMsg, exc_info=True)
                # Decide whether to stop or continue with other files
                raise FileProcessingError(errMsg) from e # Stop on first error for safety
            except Exception as e:
                errMsg = f"An unexpected error occurred saving file '{fullPath}': {e}"
                logger.error(errMsg, exc_info=True)
                raise FileProcessingError(errMsg) from e # Stop on first error

        logger.info(f"Successfully saved {len(savedFilesList)} files.")
        return savedFilesList

# TODO: Add function to read multiple file contents given a list of paths and base dir?
#       Could be useful for the LLMInterface prompt building step.
# --- END: core/file_processor.py ---