# Updated Codebase/core/file_processor.py
# --- START: core/file_processor.py ---
# core/file_processor.py ---
"""
Handles processing related to file content generated by the LLM.
Includes extracting structured data (e.g., JSON) from the LLM response
and writing updated file contents to the local disk with enhanced path validation.
"""

import logging
import json
import os
import re # For regex fallback
from typing import Dict, Optional, List, Any # Use Any for parsed data initially

# Try importing yaml for type checking if available, but don't require it globally
try:
    import yaml
    PYYAML_AVAILABLE = True
except ImportError:
    PYYAML_AVAILABLE = False
    yaml = None # Ensure yaml is None if import fails

from .exceptions import ParsingError, FileProcessingError

logger: logging.Logger = logging.getLogger(__name__)

# Define potentially problematic characters for filenames/paths (OS-dependent subset)
# Colon is problematic on Windows (except first char in drive letter C:), others common across OSes.
# Null byte is always problematic.
# Allows C:\... but flags file:name.txt or other colons.
# Updated regex to disallow colons unless it's the second character (drive letter)
# Disallows '*', '?', '"', '<', '>', '|', null byte '\0', and ':' unless it follows a drive letter at the start.
INVALID_PATH_CHARS_REGEX = re.compile(r'[*?"<>|\0]|(?<!^[a-zA-Z]):')


class FileProcessor:
    """
    Provides methods for parsing LLM output and saving generated file content.
    """
    def __init__(self: 'FileProcessor') -> None:
        """Initialises the FileProcessor."""
        logger.debug("FileProcessor initialised.")
        # No specific configuration needed at initialisation for now.

    def extractCodeBlock(self: 'FileProcessor', llmResponse: str, language: str = 'json') -> Optional[str]:
        """
        Extracts the content of the first fenced code block (e.g., ```json ... ```)
        from the LLM's response string. Uses improved line-based searching and regex fallbacks.

        Note: This implementation uses multiple strategies (line searching, regex)
        providing reasonable robustness against common LLM response variations.
        Further improvements (e.g., full markdown parsing) could be considered if
        this method proves unreliable in specific edge cases.

        Args:
            llmResponse (str): The raw response string from the LLM.
            language (str): The language identifier of the code block (e.g., 'json', 'yaml').
                            Defaults to 'json'. If empty string, matches any ``` block without language tag.

        Returns:
            Optional[str]: The extracted content within the code block (excluding fences),
                           or None if no matching block is found.
        """
        logger.debug(f"Attempting to extract '{language or 'generic'}' code block...")
        if not llmResponse: # Handle empty input
             return None

        lines: List[str] = llmResponse.splitlines()
        startFencePrefixSpecific: Optional[str] = f"```{language.lower()}" if language else None # Specific fence, lowercase
        startFenceGeneric: str = "```"
        endFence: str = "```"
        foundBlockType: Optional[str] = None # Track if specific or generic block found

        startLineIndex: int = -1
        endLineIndex: int = -1

        # --- Primary Method: Line-based search (Improved) ---
        # 1. Try finding specific language fence first
        if startFencePrefixSpecific:
            for i, line in enumerate(lines):
                stripped_line_lower = line.strip().lower()
                # Check for ```lang possibly with trailing whitespace, case-insensitive
                if stripped_line_lower.startswith(startFencePrefixSpecific):
                    # Ensure it's not something like ```jsoncpp
                    if len(stripped_line_lower) == len(startFencePrefixSpecific) or stripped_line_lower[len(startFencePrefixSpecific):].isspace():
                        startLineIndex = i
                        foundBlockType = language
                        break
        # 2. If specific not found or no language requested, try generic fence
        if startLineIndex == -1:
            for i, line in enumerate(lines):
                stripped_line = line.strip()
                # Check for exactly ``` optionally followed by non-language text (like comments) or nothing
                if stripped_line.startswith(startFenceGeneric):
                     # Check if what follows ``` looks like a language identifier or just whitespace/comment
                     potential_lang = stripped_line[3:].strip()
                     # If language was specified, we only want the generic fence if it *doesn't* have a conflicting language tag.
                     # If no language was specified (''), we accept any ``` fence.
                     if language and potential_lang and potential_lang.lower() != language.lower():
                          continue # Found ``` but with a different language tag, skip
                     # If language was specified and it matches, we should have found it in step 1.
                     # If language is empty, accept any ``` fence start.
                     # Heuristic: If there's non-whitespace after ```, it might be a language tag.
                     # For generic match (language=''), we prefer ``` on its own line.
                     if language == '' and len(potential_lang) > 0:
                          # If looking for generic block, skip if it looks like ```python
                          continue

                     startLineIndex = i
                     foundBlockType = 'generic' # Mark as generic (or requested language if language='')
                     break

        # If a start fence was found, look for the end fence
        if startLineIndex != -1:
            for i in range(startLineIndex + 1, len(lines)):
                # Check for exactly ``` with optional leading/trailing whitespace
                if lines[i].strip() == endFence:
                    endLineIndex = i
                    break

        # Extract content if both fences found using line search
        if startLineIndex != -1 and endLineIndex != -1 and endLineIndex > startLineIndex:
            blockContentLines = lines[startLineIndex+1:endLineIndex]
            extractedContent = "\n".join(blockContentLines).strip() # Join lines and strip outer whitespace
            # Use foundBlockType for logging
            log_block_type = foundBlockType if foundBlockType and foundBlockType != 'generic' else (language or 'generic')
            logger.info(f"Successfully extracted '{log_block_type}' code block using line search. Length: {len(extractedContent)}")
            return extractedContent
        else:
            # Log warning only if line search failed but regex might work.
            log_fallback_warning = True
            if startLineIndex == -1: # No start fence found at all
                log_fallback_warning = False

            if log_fallback_warning:
                 # FIX: Log warning indicating fallback attempt.
                 logger.warning("Could not find complete fenced code block using line search. Attempting regex fallback...")

            # --- Fallback Method: Regex (Improved) ---
            extractedContent: Optional[str] = None

            # Regex 1: Try specific language first (more flexible whitespace)
            # Matches ``` optional_whitespace LANG optional_whitespace newline CONTENT newline optional_whitespace ```
            # Captures CONTENT. Case-insensitive for LANG. DOTALL makes . match newlines. Non-greedy content capture.
            if language:
                lang_pattern_re = re.escape(language)
                # Pattern handles optional newline after language tag
                pattern_re_specific = rf"```\s*{lang_pattern_re}\s*\n?(.*?)\n?\s*```"
                flags_re = re.DOTALL | re.IGNORECASE
                match = re.search(pattern_re_specific, llmResponse, flags_re)
                if match:
                    extractedContent = match.group(1).strip() # Capture group 1 contains the content
                    foundBlockType = language
                    logger.info(f"Successfully extracted '{foundBlockType}' code block using specific regex fallback. Length: {len(extractedContent)}")
                    return extractedContent

            # Regex 2: Try generic block if specific failed or wasn't requested
            # Matches ``` optional_whitespace (optional LANG) optional_whitespace newline CONTENT newline optional_whitespace ```
            # Captures CONTENT.
            # FIX: Improved regex to be non-greedy and handle optional language tag better.
            if extractedContent is None:
                # Non-greedy match for content (.*?); optional language tag `[\w\-]*` that doesn't consume content start.
                pattern_re_generic = r"```[^\S\n]*(?:[\w\-]+)?[^\S\n]*\n(.*?)```" # Simpler: Find ```, skip optional lang line, capture until next ```
                flags_re = re.DOTALL
                match = re.search(pattern_re_generic, llmResponse, flags_re)
                if match:
                    extractedContent = match.group(1).strip() # Group 1 is the content
                    # This might still extract content from a block like ```python ... ```
                    # if the specific language search failed. This is often desired fallback behaviour.
                    foundBlockType = 'generic (fallback)'
                    # FIX: Log info message for successful fallback regex match.
                    logger.info(f"Successfully extracted '{foundBlockType}' code block using generic regex fallback. Length: {len(extractedContent)}")
                    return extractedContent

            # If all methods fail
            logger.error(f"Could not find a fenced code block matching '```{language or ''}' or generic '```' using any method.")
            return None


    def _is_safe_relative_path(self: 'FileProcessor', path: Optional[str]) -> bool:
        """
        Validates if a given path string is a safe relative path component.
        Checks for non-strings, empty strings, absolute paths (Unix, Windows, UNC),
        directory traversal ('..'), and problematic characters.

        Args:
            path (Optional[str]): The path string to validate.

        Returns:
            bool: True if the path is considered safe, False otherwise.
        """
        if not isinstance(path, str) or not path.strip(): # Check for non-string or empty/whitespace-only
            logger.warning(f"Path validation failed: Path is not a non-empty string (received: {repr(path)}).")
            return False

        # Trim whitespace that might interfere with checks
        path = path.strip()

        # 1. Check for absolute paths more robustly
        if os.path.isabs(path):
             logger.warning(f"Path validation failed: Path '{path}' appears absolute (os.path.isabs).")
             return False
        # Explicitly check for Windows UNC paths
        if path.startswith('\\\\') or path.startswith('//'): # Check both common UNC prefixes
             logger.warning(f"Path validation failed: Path '{path}' appears to be a UNC path.")
             return False
        # Prevent drive-relative paths like C:foo.txt (without backslash)
        if len(path) > 1 and path[1] == ':' and path[0].isalpha():
             logger.warning(f"Path validation failed: Path '{path}' appears to be a drive-relative path (e.g., 'C:file').")
             return False

        # 2. Check for '..' components to prevent directory traversal
        # Use os.path.normpath aggressively early to collapse paths like 'a/b/../c' -> 'a/c'
        try:
            # Normalize using forward slashes first for consistency before splitting
            normalized_path = os.path.normpath(path.replace('\\', '/'))

            # After normalization, check if it starts with '..' or contains '/../' or is exactly '..'
            # Check components directly after splitting by '/'
            if '..' in normalized_path.split('/'):
                 logger.warning(f"Path validation failed: Path '{path}' contains '..' component (checked after normalization: '{normalized_path}').")
                 return False
            # Also check if the normalized path *starts* with '..' which normpath might leave if root is reached
            if normalized_path.startswith('..'):
                 logger.warning(f"Path validation failed: Normalized path '{normalized_path}' (from '{path}') starts with '..'.")
                 return False

        except (ValueError, TypeError) as e: # Catch normalization errors
            logger.warning(f"Path validation failed: Error normalizing path '{path}' for traversal check: {e}")
            return False


        # 3. Check for potentially problematic characters using updated regex
        invalid_char_match = INVALID_PATH_CHARS_REGEX.search(path)
        if invalid_char_match:
            invalid_char = invalid_char_match.group(0)
            # FIX: Use repr() for safer logging of potentially non-printable characters like null byte.
            logger.warning(f"Path validation failed: Path '{path}' contains invalid character ({repr(invalid_char)}).")
            return False

        # Path appears to be a safe relative path component
        return True


    def parseStructuredOutput(self: 'FileProcessor', structuredDataString: Optional[str], format: str = 'json') -> Dict[str, str]:
        """
        Parses the extracted structured data string (e.g., JSON, YAML) into a dictionary.
        Validates that the output is a dictionary mapping safe relative filenames (str) to content (str).

        Args:
            structuredDataString (Optional[str]): The string content extracted from the code block. Can be None or empty.
            format (str): The expected format ('json' or 'yaml'). Defaults to 'json'.

        Returns:
            Dict[str, str]: A dictionary where keys are relative file paths and values
                            are the corresponding file contents. Returns empty dict if input is None/empty.

        Raises:
            ParsingError: If the string is not valid for the specified format, if
                          the parsed structure is not Dict[str, str], or if keys are
                          not safe relative paths.
            NotImplementedError: If an unsupported format is requested.
        """
        # Handle empty input gracefully
        if not structuredDataString:
            logger.info("Received empty or None structured data string. Returning empty dictionary.")
            return {}

        # Strip outer whitespace which might interfere with parsing
        structuredDataString = structuredDataString.strip()
        if not structuredDataString: # Check again after stripping
             logger.info("Received whitespace-only structured data string. Returning empty dictionary.")
             return {}

        logger.info(f"Parsing structured output as '{format}'. Length: {len(structuredDataString)}")
        parsedData: Any # Keep Any initially, type check later

        try:
            if format == 'json':
                # Try to parse directly first
                try:
                    parsedData = json.loads(structuredDataString)
                except json.JSONDecodeError as e:
                    logger.warning(f"Initial JSON parsing failed ({e}). Attempting to find and parse JSON object/array within the string...")
                    # Regex to find content between the first '{' or '[' and the last matching '}' or ']'
                    # This is heuristic and might fail on complex nested structures or strings containing brackets/braces.
                    match = re.search(r"(\{.*\}|\[.*\])", structuredDataString, re.DOTALL)
                    if match:
                        potential_json = match.group(0)
                        logger.debug(f"Attempting to parse potentially stripped content (first 100 chars): {potential_json[:100]}...")
                        try:
                            parsedData = json.loads(potential_json)
                            logger.info("Successfully parsed JSON after attempting to strip surrounding text.")
                        except json.JSONDecodeError as inner_e:
                            # If still fails, raise original error but mention the attempt
                            errMsg = f"Invalid JSON detected: {e}. Attempting to strip surrounding text also failed ({inner_e}). Check the LLM response format."
                            logger.error(errMsg, exc_info=False)
                            raise ParsingError(errMsg) from e
                    else:
                        # Could not find start/end markers, raise original error
                        errMsg = f"Invalid JSON detected: {e}. Could not find JSON object/array markers to attempt stripping."
                        logger.error(errMsg, exc_info=False)
                        raise ParsingError(errMsg) from e

            elif format == 'yaml':
                # FIX: Correctly handle PyYAML unavailability
                if not PYYAML_AVAILABLE or yaml is None:
                    errMsg = "Parsing format 'yaml' requested, but PyYAML library is not installed or failed to import. Please install it (`pip install pyyaml`)."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)
                # Use safe_load which is generally recommended
                # safe_load can return None for empty input, handle this.
                parsedData = yaml.safe_load(structuredDataString)
            else:
                raise NotImplementedError(f"Parsing for format '{format}' is not implemented.")

            # --- Validation ---
            # FIX: Handle case where parsing results in None (e.g., empty YAML input)
            if parsedData is None:
                logger.warning(f"Parsing result for '{format}' was None (e.g., empty input). Returning empty dictionary.")
                return {}

            if not isinstance(parsedData, dict):
                errMsg = f"Parsed data is not a dictionary as expected. Found type: {type(parsedData).__name__}"
                logger.error(errMsg)
                raise ParsingError(errMsg)

            validatedData: Dict[str, str] = {}
            # Check key/value types and path safety
            for key, value in parsedData.items():
                # Key Validation (must be string and safe relative path)
                # FIX: Ensure _is_safe_relative_path is robust for non-string keys before calling
                if not isinstance(key, str):
                    # FIX: Handle non-string keys gracefully before passing to _is_safe_relative_path
                    # Match test expectation for error message formatting. Use repr() for clarity.
                    # Example test regex might expect: "Invalid structure: Dictionary key `123` is not a string."
                    # Let's generate: "Invalid structure: Dictionary key 123 (type: int) is not a string."
                    errMsg = f"Invalid structure: Dictionary key {repr(key)} (type: {type(key).__name__}) is not a string."
                    logger.error(errMsg)
                    # To match the specific test failure "does not match \"Invalid structure: Dictionary key '123' is not a safe relative path.\""
                    # It seems the *test* is asserting the wrong message format if the key is non-string.
                    # The correct error here is that the key isn't a string. Let's raise *that* error.
                    # The test `test_parseStructuredOutput_json_wrongStructure_badKeys_type` needs adjustment.
                    raise ParsingError(errMsg)


                if not self._is_safe_relative_path(key):
                    # _is_safe_relative_path logs the specific reason
                    errMsg = f"Invalid structure: Dictionary key '{key}' is not a safe relative path."
                    # No need to log again here, _is_safe_relative_path already did.
                    raise ParsingError(errMsg) # Raise error if validation fails

                # Value Validation (must be string)
                if not isinstance(value, str):
                    errMsg = f"Invalid structure: Value for key '{key}' is not a string (type: {type(value).__name__})."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)

                validatedData[key] = value # Add to validated dict

            logger.info(f"Successfully parsed and validated '{format}' data. Found {len(validatedData)} file entries.")
            return validatedData

        except json.JSONDecodeError as e:
            # This path is less likely now with the inner handling, but keep as fallback
            errMsg = f"Invalid JSON detected: {e}. Check the LLM response format."
            logger.error(errMsg, exc_info=False) # Log basic error, no need for full stack trace usually
            raise ParsingError(errMsg) from e
        except yaml.YAMLError as e:
            errMsg = f"Invalid YAML detected: {e}. Check the LLM response format."
            logger.error(errMsg, exc_info=False)
            raise ParsingError(errMsg) from e
        except NotImplementedError as e:
            logger.error(str(e))
            raise e # Re-raise NotImplementedError
        except ParsingError as e: # Re-raise validation errors
            raise e
        except Exception as e: # Catch other unexpected errors
            errMsg = f"An unexpected error occurred during parsing: {e}"
            logger.error(errMsg, exc_info=True) # Log full trace for unexpected errors
            raise ParsingError(errMsg) from e

    def saveFilesToDisk(self: 'FileProcessor', outputDir: str, fileData: Dict[str, str]) -> List[str]:
        """
        Saves the file contents from the parsed dictionary to the specified output directory.
        Performs rigorous path validation before writing. Creates necessary subdirectories
        and overwrites existing files.

        Args:
            outputDir (str): The base directory (e.g., the cloned repo path) where files should be saved.
                             Must be an existing directory.
            fileData (Dict[str, str]): The dictionary mapping validated relative file paths to their content.

        Returns:
            List[str]: A list of the relative paths of the files that were successfully saved.

        Raises:
            FileProcessingError: If `outputDir` is not a valid directory, if any relative path
                                 fails safety checks during this stage, or if there are errors
                                 creating directories or writing files (e.g., permissions).
        """
        logger.info(f"Saving {len(fileData)} files to base directory: {outputDir}")
        savedFilesList: List[str] = []

        if not isinstance(outputDir, str) or not outputDir.strip():
            errMsg = "Output directory path must be a non-empty string."
            logger.error(errMsg)
            raise FileProcessingError(errMsg)

        outputDir = outputDir.strip()

        # Resolve outputDir once for reliable path comparisons and existence check
        try:
            resolvedOutputDir = os.path.realpath(outputDir)
        except OSError as e:
            errMsg = f"Could not resolve real path for output directory '{outputDir}': {e}"
            logger.error(errMsg)
            raise FileProcessingError(errMsg) from e

        if not os.path.isdir(resolvedOutputDir):
            errMsg = f"Resolved output directory '{resolvedOutputDir}' (from '{outputDir}') does not exist or is not a directory."
            logger.error(errMsg)
            raise FileProcessingError(errMsg)

        if not fileData:
            logger.info("Received empty file data dictionary. No files to save.")
            return []

        for relativePath, content in fileData.items():
            # --- Final Path Validation before writing ---
            # Path should have been validated by parseStructuredOutput, but double-check.
            if not self._is_safe_relative_path(relativePath):
                 # FIX: Raise FileProcessingError directly here for path issues found at save time.
                 errMsg = f"Skipping file due to unsafe relative path discovered just before saving: '{relativePath}'"
                 logger.error(errMsg + ". This should ideally have been caught during parsing.")
                 raise FileProcessingError(errMsg) # Stop processing if unsafe path found here

            try:
                # Combine and normalize the path using the *resolved* output directory
                # Use os.path.normpath to clean up separators, etc.
                fullPath_normalized = os.path.normpath(os.path.join(resolvedOutputDir, relativePath))

                # Critical Check: Use realpath again on the *final candidate path* and ensure it's within the output dir.
                resolvedFullPath = os.path.realpath(fullPath_normalized)

                # The most reliable check: the resolved full path must start with the resolved output directory path + separator
                # Ensure resolvedOutputDir ends with a separator for accurate startswith check
                base_check_path = os.path.join(resolvedOutputDir, '') # Adds separator if needed

                if not resolvedFullPath.startswith(base_check_path):
                    errMsg = (f"Path traversal detected after resolving final path for '{relativePath}'. "
                              f"Target '{resolvedFullPath}' is outside base directory '{resolvedOutputDir}'. Aborting save.")
                    logger.critical(errMsg) # Use critical level for security issues
                    raise FileProcessingError(errMsg) # Raise error directly

                # Optional: Check if resolved path is identical to base path (trying to overwrite dir)
                if resolvedFullPath == resolvedOutputDir:
                    errMsg = f"Attempting to write directly to the output directory '{resolvedOutputDir}' (relative path was likely empty or '.'). Aborting save."
                    logger.critical(errMsg)
                    raise FileProcessingError(errMsg)

            except OSError as e: # Catch errors during path manipulation/resolution (like realpath)
                errMsg = f"OS error resolving or checking final path for '{relativePath}' within '{resolvedOutputDir}': {e}"
                logger.error(errMsg)
                raise FileProcessingError(errMsg) from e
            except Exception as e: # Catch unexpected errors during path checks
                errMsg = f"Unexpected error validating final path for '{relativePath}': {e}"
                logger.error(errMsg, exc_info=True)
                raise FileProcessingError(errMsg) from e

            # If checks pass, proceed to save
            logger.debug(f"Validated path. Preparing to save file: {resolvedFullPath}")

            try:
                # Create parent directories if they don't exist
                fileDir: str = os.path.dirname(resolvedFullPath)
                # Check if directory needs creation (and isn't the output dir itself)
                if fileDir != resolvedOutputDir: # Avoid trying to create the base dir
                    os.makedirs(fileDir, exist_ok=True)
                    logger.debug(f"Ensured directory exists: {fileDir}")

                # Write the file content (overwrite if exists)
                with open(resolvedFullPath, 'w', encoding='utf-8') as fileHandle:
                    fileHandle.write(content)

                savedFilesList.append(relativePath) # Log the original relative path
                logger.debug(f"Successfully wrote file: {resolvedFullPath}")

            except OSError as e:
                errMsg = f"OS error writing file '{resolvedFullPath}': {e}"
                logger.error(errMsg, exc_info=True)
                # Stop on first error for safety
                raise FileProcessingError(errMsg) from e
            except Exception as e:
                errMsg = f"An unexpected error occurred saving file '{resolvedFullPath}': {e}"
                logger.error(errMsg, exc_info=True)
                # Stop on first error
                raise FileProcessingError(errMsg) from e

        logger.info(f"Successfully saved {len(savedFilesList)} files.")
        return savedFilesList

# --- END: core/file_processor.py ---