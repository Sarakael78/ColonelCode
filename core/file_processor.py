# core/file_processor.py
"""
Handles processing related to file content generated by the LLM.
Includes extracting structured data (e.g., JSON) from the LLM response
and writing updated file contents to the local disk with enhanced path validation.
"""

import logging
import json
import os
import re # For regex fallback
from typing import Dict, Optional, List, Any, Union # Added Union

# Try importing yaml for type checking if available, but don't require it globally
try:
    import yaml
    PYYAML_AVAILABLE = True
except ImportError:
    PYYAML_AVAILABLE = False
    yaml = None # Ensure yaml is None if import fails

from .exceptions import ParsingError, FileProcessingError

logger: logging.Logger = logging.getLogger(__name__)

# Define potentially problematic characters for filenames/paths (OS-dependent subset)
# Allows C:\... but flags file:name.txt or other colons.
# Updated regex to disallow colons unless it's the second character (drive letter)
# Disallows '*', '?', '"', '<', '>', '|', null byte '\0', and ':' unless it follows a drive letter at the start.
INVALID_PATH_CHARS_REGEX = re.compile(r'[*?"<>|\0]|(?<!^[a-zA-Z]):')


class FileProcessor:
    """
    Provides methods for parsing LLM output and saving generated file content.
    """
    def __init__(self: 'FileProcessor') -> None:
        """Initialises the FileProcessor."""
        logger.debug("FileProcessor initialised.")
        # No specific configuration needed at initialisation for now.

    def extractCodeBlock(self: 'FileProcessor', llmResponse: str, language: str = 'json') -> Optional[str]:
        """
        Extracts the content of the first fenced code block (e.g., ```json ... ``` or ```python ... ```)
        from the LLM's response string. Uses improved line-based searching and regex fallbacks.

        It prioritizes finding a block matching the specified `language`. If not found,
        it falls back to finding the *first* fenced code block regardless of its language tag.

        Args:
            llmResponse (str): The raw response string from the LLM.
            language (str): The preferred language identifier of the code block (e.g., 'json', 'yaml').
                            Defaults to 'json'. If the specific language block isn't found,
                            it will look for any other fenced block. If language is an empty string,
                            it specifically looks for blocks without any language tag (```\n...\n```).

        Returns:
            Optional[str]: The extracted content within the code block (excluding fences),
                           or None if no fenced code block is found.
        """
        logger.debug(f"Attempting to extract '{language or 'generic/any'}' code block...")
        if not llmResponse: return None

        lines: List[str] = llmResponse.splitlines()
        startFencePrefixSpecific: Optional[str] = f"```{language.lower()}" if language else None
        startFenceGenericPrefix: str = "```"
        endFence: str = "```"
        foundBlockType: Optional[str] = None
        startLineIndex: int = -1
        endLineIndex: int = -1

        # --- Primary Method: Line-based search ---
        # 1. Try specific language fence first if a language is specified
        if startFencePrefixSpecific:
            for i, line in enumerate(lines):
                stripped_line_lower = line.strip().lower()
                # Check if line starts with ```language and has nothing or only whitespace after it
                if stripped_line_lower.startswith(startFencePrefixSpecific) and \
                   (len(stripped_line_lower) == len(startFencePrefixSpecific) or stripped_line_lower[len(startFencePrefixSpecific):].isspace()):
                    startLineIndex = i
                    foundBlockType = language # Found the specifically requested language
                    logger.debug(f"Found specific language fence '```{language}' at line {i+1}.")
                    break # Found the preferred block, stop searching

        # 2. If specific not found (or not requested), search for *any* code block fence (``` or ```otherlang)
        #    This handles cases where the LLM might use a different tag (e.g., ```python) even if 'json' was expected.
        if startLineIndex == -1:
            for i, line in enumerate(lines):
                stripped_line = line.strip()
                if stripped_line.startswith(startFenceGenericPrefix):
                    # Check if it's just ``` or ``` followed by something
                    potential_lang = stripped_line[len(startFenceGenericPrefix):].strip()
                    # Check if the rest of the line is empty or whitespace (or a language tag)
                    if potential_lang or len(stripped_line) == len(startFenceGenericPrefix):
                        # If language='' was specifically requested (meaning ```\n), skip blocks with tags.
                        if language == '' and potential_lang:
                            logger.debug(f"Skipping block with language '{potential_lang}' at line {i+1} because generic (no tag) block was requested.")
                            continue

                        # Found a generic block (```) or a block with a different language tag.
                        startLineIndex = i
                        foundBlockType = potential_lang if potential_lang else 'generic' # Store what was found
                        logger.debug(f"Found first available code block fence ('```{foundBlockType}') at line {i+1} (fallback).")
                        break # Found the first available block, stop searching

        # Find the corresponding end fence
        if startLineIndex != -1:
            for i in range(startLineIndex + 1, len(lines)):
                if lines[i].strip() == endFence:
                    endLineIndex = i
                    break

        # Extract content if a complete block was found by line search
        if startLineIndex != -1 and endLineIndex != -1 and endLineIndex > startLineIndex:
            blockContentLines = lines[startLineIndex+1:endLineIndex]
            extractedContent = "\n".join(blockContentLines).strip()
            # Determine log message based on what was found vs requested
            log_block_type_msg = f"'{foundBlockType}'"
            if language and foundBlockType != language.lower():
                log_block_type_msg += f" (found via fallback, '{language}' requested)"
            elif not language and foundBlockType != 'generic':
                 log_block_type_msg += f" (found via fallback, generic/any requested)"
            elif foundBlockType == 'generic':
                 log_block_type_msg += " (no language tag)"

            logger.info(f"Successfully extracted code block {log_block_type_msg} using line search. Length: {len(extractedContent)}")
            return extractedContent
        else:
            # --- Fallback Method: Regex ---
            # Log warning only if line search looked promising (found start) but failed to find end
            if startLineIndex != -1 and endLineIndex == -1:
                logger.warning(f"Found start fence '```{foundBlockType or ''}' but no end fence using line search. Attempting regex fallback...")
            else:
                logger.debug("Could not find any code block fence using line search. Attempting regex fallback...")


            extractedContent: Optional[str] = None
            # Regex 1: Try specific language first (if specified)
            # Use re.IGNORECASE for the language tag in the regex
            if language:
                lang_pattern_re = re.escape(language)
                # Pattern: ``` optional_whitespace LANGUAGE optional_whitespace optional_newline CONTENT optional_newline ```
                pattern_re_specific = rf"```{re.escape(language)}\s*\n?(.*?)\n?\s*```"
                flags_re = re.DOTALL | re.IGNORECASE # DOTALL allows '.' to match newlines, IGNORECASE for language tag
                match = re.search(pattern_re_specific, llmResponse, flags_re)
                if match:
                    extractedContent = match.group(1).strip()
                    logger.info(f"Successfully extracted '{language}' code block using specific regex fallback. Length: {len(extractedContent)}")
                    return extractedContent

            # Regex 2: Generic block (matches ``` or ```any_language) - This is the main fallback
            if extractedContent is None:
                # Pattern: ``` optional_whitespace [optional language tag] optional_whitespace newline CONTENT ```
                pattern_re_generic = r"```\h*(?:[\w\-]+)?\h*\s*\n?(.*?)\n?\s*```"
                flags_re = re.DOTALL # DOTALL is crucial here
                match = re.search(pattern_re_generic, llmResponse, flags_re)
                if match:
                    extractedContent = match.group(1).strip()
                    # Try to determine the language tag found by the regex for logging
                    full_match_text = match.group(0)
                    first_line = full_match_text.split('\n', 1)[0]
                    lang_tag_found = first_line.strip()[3:].strip() or 'generic (no tag)'

                    log_block_type_msg = f"'{lang_tag_found}'"
                    if language and lang_tag_found != language.lower():
                         log_block_type_msg += f" (found via generic regex fallback, '{language}' requested)"
                    elif not language and lang_tag_found != 'generic (no tag)':
                         log_block_type_msg += f" (found via generic regex fallback, generic/any requested)"

                    logger.info(f"Successfully extracted code block {log_block_type_msg} using generic regex fallback. Length: {len(extractedContent)}")
                    return extractedContent

            # If neither line search nor regex worked
            logger.error(f"Could not find any fenced code block (```...```) using any method. Language requested: '{language or 'generic/any'}'.")
            return None


    def _is_safe_relative_path(self: 'FileProcessor', path: Optional[str]) -> bool:
        """
        Validates if a given path string is a safe relative path component.
        Checks for non-strings, empty strings, absolute paths (Unix, Windows, UNC),
        directory traversal ('..'), and problematic characters.

        Args:
            path (Optional[str]): The path string to validate.

        Returns:
            bool: True if the path is considered safe, False otherwise.
        """
        if not isinstance(path, str) or not path.strip():
            logger.warning(f"Path validation failed: Path is not a non-empty string (received: {repr(path)}).")
            return False
        path = path.strip()
        if os.path.isabs(path):
            logger.warning(f"Path validation failed: Path '{path}' appears absolute (os.path.isabs).")
            return False
        if path.startswith('\\\\') or path.startswith('//'):
            logger.warning(f"Path validation failed: Path '{path}' appears to be a UNC path.")
            return False
        # Check for potential drive letters without being absolute (e.g., "C:file.txt" - invalid relative path)
        # Allows "C:\..." (caught by isabs) but flags "C:file"
        if len(path) > 1 and path[1] == ':' and path[0].isalpha() and not os.path.isabs(path):
            logger.warning(f"Path validation failed: Path '{path}' appears to be a drive-relative path (e.g., 'C:file').")
            return False
        try:
            # Normalize path separators to '/' for consistent '..' check
            normalized_path = os.path.normpath(path.replace('\\', '/'))
            # Check for '..' components in the normalized path segments
            if '..' in normalized_path.split('/') or normalized_path.startswith('../') or normalized_path == '..':
                logger.warning(f"Path validation failed: Path '{path}' contains '..' component (checked after normalization: '{normalized_path}').")
                return False
        except (ValueError, TypeError) as e: # Catch errors during normalization (e.g., null bytes)
            logger.warning(f"Path validation failed: Error normalizing path '{path}' for traversal check: {e}")
            return False
        # Check for other invalid characters using regex
        invalid_char_match = INVALID_PATH_CHARS_REGEX.search(path)
        if invalid_char_match:
            invalid_char = invalid_char_match.group(0)
            logger.warning(f"Path validation failed: Path '{path}' contains invalid character ({repr(invalid_char)}).")
            return False
        return True

    # --- UPDATED parseStructuredOutput ---
    def parseStructuredOutput(self: 'FileProcessor', structuredDataString: Optional[str], format: str = 'json') -> Dict[str, str]:
        """
        Parses the extracted structured data string (e.g., JSON, YAML) into a dictionary.
        Validates that the output is a dictionary mapping safe relative filenames (str)
        to content (str), allowing for content nested within a {"content": "..."} dictionary.
        Includes fallback logic for attempting to parse JSON even if surrounded by other text.
        Provides enhanced error messages for JSON parsing failures.

        Args:
            structuredDataString (Optional[str]): The raw string extracted from the LLM response,
                                                  expected to contain the structured data.
            format (str): The expected format ('json' or 'yaml'). Defaults to 'json'.

        Returns:
            Dict[str, str]: A dictionary mapping validated relative file paths to their content strings.

        Raises:
            ParsingError: If the string cannot be parsed, if the structure is invalid (not dict,
                          unsafe keys, invalid value types), or if the format is unsupported.
            NotImplementedError: If the requested format is not 'json' or 'yaml'.
        """
        if not structuredDataString:
            logger.info("Received empty or None structured data string. Returning empty dictionary.")
            return {}
        structuredDataString = structuredDataString.strip()
        if not structuredDataString:
            logger.info("Received whitespace-only structured data string. Returning empty dictionary.")
            return {}

        logger.info(f"Parsing structured output as '{format}'. Length: {len(structuredDataString)}")
        parsedData: Any

        try:
            # --- Parsing Logic (with JSON fallback) ---
            if format == 'json':
                try:
                    parsedData = json.loads(structuredDataString)
                except json.JSONDecodeError as e:
                    # Fallback logic for JSON surrounded by text
                    logger.warning(f"Initial JSON parsing failed ({e.msg} at char {e.pos}). Attempting JSON extraction fallback...")
                    potential_json = None
                    try:
                        # Try finding the outermost curly braces or square brackets
                        first_brace = structuredDataString.find('{')
                        first_bracket = structuredDataString.find('[')
                        start_index = -1
                        start_char = ''
                        end_char = ''

                        # Determine the starting character and index
                        if first_brace != -1 and (first_bracket == -1 or first_brace < first_bracket):
                            start_index = first_brace
                            start_char = '{'
                            end_char = '}'
                        elif first_bracket != -1:
                            start_index = first_bracket
                            start_char = '['
                            end_char = ']'
                        else:
                            # Neither '{' nor '[' found, cannot extract
                            start_index = -1


                        if start_index != -1:
                            # Find the matching closing bracket/brace, considering nesting
                            nesting_level = 0
                            end_index = -1
                            for i in range(start_index, len(structuredDataString)):
                                char = structuredDataString[i]
                                if char == start_char: # Opening char
                                    nesting_level += 1
                                elif char == end_char: # Closing char
                                    nesting_level -= 1
                                    if nesting_level == 0:
                                        end_index = i
                                        break # Found the matching end
                            if end_index != -1:
                                potential_json = structuredDataString[start_index : end_index + 1]
                                logger.debug(f"Fallback extracted potential JSON substring from index {start_index} to {end_index}.")
                            else:
                                logger.warning(f"Fallback found opening '{start_char}' at index {start_index} but no matching closing '{end_char}'.")
                                potential_json = None
                        else:
                             logger.warning("Fallback could not find starting '{' or '[' in the string.")
                             potential_json = None

                    except Exception as fallback_err: # Catch errors during the fallback logic itself
                        logger.error(f"Error during JSON fallback extraction logic: {fallback_err}", exc_info=True)
                        potential_json = None

                    # Try parsing the extracted substring if found
                    if potential_json:
                        logger.debug("Attempting to parse content found by fallback logic...")
                        try:
                            parsedData = json.loads(potential_json)
                            logger.info("Successfully parsed JSON using fallback extraction logic.")
                        except json.JSONDecodeError as inner_e:
                            # Parsing the extracted part also failed
                            errMsg = (f"Invalid JSON detected: {e.msg} (at char {e.pos}). "
                                      f"Attempting to parse extracted content also failed: {inner_e.msg} (at char {inner_e.pos}). "
                                      f"Check LLM response format.")
                            logger.error(errMsg, exc_info=False) # Log original and inner error details
                            # Optionally include a snippet of the failed string for debugging
                            snippet = structuredDataString[max(0, e.pos-15):e.pos+15]
                            logger.error(f"    Nearby text (original): ...{snippet}...")
                            snippet_inner = potential_json[max(0, inner_e.pos-15):inner_e.pos+15]
                            logger.error(f"    Nearby text (extracted): ...{snippet_inner}...")
                            raise ParsingError(errMsg) from inner_e
                    else:
                        # Fallback couldn't find markers or extract a substring
                        errMsg = (f"Invalid JSON detected: {e.msg} (at char {e.pos}). "
                                  f"Could not find valid JSON object/array markers using fallback logic. "
                                  f"Check LLM response format.")
                        logger.error(errMsg, exc_info=False)
                        snippet = structuredDataString[max(0, e.pos-15):e.pos+15]
                        logger.error(f"    Nearby text: ...{snippet}...")
                        raise ParsingError(errMsg) from e

            elif format == 'yaml':
                # YAML Parsing
                if not PYYAML_AVAILABLE or yaml is None:
                    errMsg = "Parsing format 'yaml' requested, but PyYAML library is not installed. Install it (`pip install pyyaml`)."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)
                # Add try-except for yaml parsing errors
                try:
                    parsedData = yaml.safe_load(structuredDataString)
                except yaml.YAMLError as e:
                    errMsg = f"Invalid YAML detected: {e}. Check LLM response format."
                    if hasattr(e, 'problem_mark'):
                        errMsg += f" Error near line {e.problem_mark.line + 1}, column {e.problem_mark.column + 1}."
                    logger.error(errMsg, exc_info=False)
                    raise ParsingError(errMsg) from e
            else:
                raise NotImplementedError(f"Parsing for format '{format}' is not implemented.")

            # --- Structure and Value Validation ---
            if parsedData is None:
                # Handle cases where parsing results in None (e.g., empty YAML string)
                logger.warning(f"Parsing result for '{format}' was None (input might have been empty or only comments). Returning empty dictionary.")
                return {}
            if not isinstance(parsedData, dict):
                errMsg = f"Parsed data is not a dictionary as expected. Found type: {type(parsedData).__name__}"
                logger.error(errMsg)
                raise ParsingError(errMsg)

            validatedData: Dict[str, str] = {}
            for key, value in parsedData.items():
                # Validate Key (File Path)
                if not isinstance(key, str):
                    errMsg = f"Invalid structure: Dictionary key {repr(key)} (type: {type(key).__name__}) is not a string."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)
                if not self._is_safe_relative_path(key):
                    # Error message generated within _is_safe_relative_path
                    errMsg = f"Invalid structure: Dictionary key '{key}' is not a safe relative path."
                    # Raise error here to stop processing
                    raise ParsingError(errMsg) # _is_safe_relative_path already logged details

                # Validate and Extract Value (File Content)
                file_content_str: Optional[str] = None
                if isinstance(value, str):
                    # Original expected format: value is the content string directly
                    file_content_str = value
                elif isinstance(value, dict) and 'content' in value and isinstance(value['content'], str):
                    # New format: value is dict with a 'content' string key
                    file_content_str = value['content']
                    logger.debug(f"Extracted content for '{key}' from nested dictionary.")
                    # Optionally log or use value.get('operation') here if needed
                else:
                    # Invalid value type
                    errMsg = f"Invalid structure: Value for key '{key}' is not a string or a dictionary with a 'content' string (type: {type(value).__name__})."
                    logger.error(errMsg)
                    raise ParsingError(errMsg)

                validatedData[key] = file_content_str # Store the extracted content string

            logger.info(f"Successfully parsed and validated '{format}' data. Found {len(validatedData)} file entries.")
            return validatedData

        # --- Exception Handling ---
        except (json.JSONDecodeError, yaml.YAMLError) as e: # Catch parsing errors again if missed during fallback/validation
            # This shouldn't be reached if the logic above is correct, but handle defensively.
            error_type = type(e).__name__
            errMsg = f"Invalid {format.upper()} detected during final check: {e}. Check LLM response format."
            logger.error(errMsg, exc_info=False) # Log less detail here as it should have been caught earlier
            raise ParsingError(errMsg) from e
        except NotImplementedError as e:
            logger.error(str(e))
            raise e # Re-raise
        except ParsingError as e:
            # Re-raise specific parsing errors from validation steps
            raise e
        except Exception as e:
            # Catch any other unexpected errors during parsing/validation
            errMsg = f"An unexpected error occurred during parsing/validation of {format} data: {e}"
            logger.error(errMsg, exc_info=True)
            raise ParsingError(errMsg) from e
    # --- END UPDATE ---

    def saveFilesToDisk(self: 'FileProcessor', outputDir: str, fileData: Dict[str, str]) -> List[str]:
        """
        Saves the file contents from the parsed dictionary to the specified output directory.
        Performs rigorous path validation before writing. Creates necessary subdirectories
        and overwrites existing files.

        Args:
            outputDir (str): The base directory (e.g., the cloned repo path) where files should be saved.
                             Must be an existing directory.
            fileData (Dict[str, str]): The dictionary mapping validated relative file paths to their content.

        Returns:
            List[str]: A list of the relative paths of the files that were successfully saved.

        Raises:
            FileProcessingError: If `outputDir` is not a valid directory, if any relative path
                                 fails safety checks during this stage, or if there are errors
                                 creating directories or writing files (e.g., permissions).
        """
        logger.info(f"Saving {len(fileData)} files to base directory: {outputDir}")
        savedFilesList: List[str] = []

        if not isinstance(outputDir, str) or not outputDir.strip():
            errMsg = "Output directory path must be a non-empty string."
            logger.error(errMsg)
            raise FileProcessingError(errMsg)
        outputDir = outputDir.strip()

        try:
            # Resolve symlinks etc. for the output directory path
            resolvedOutputDir = os.path.realpath(outputDir)
        except OSError as e:
            errMsg = f"Could not resolve real path for output directory '{outputDir}': {e}"
            logger.error(errMsg)
            raise FileProcessingError(errMsg) from e

        # Check if the resolved path exists and is actually a directory
        if not os.path.isdir(resolvedOutputDir):
            errMsg = f"Resolved output directory '{resolvedOutputDir}' (from '{outputDir}') does not exist or is not a directory."
            logger.error(errMsg)
            raise FileProcessingError(errMsg)

        if not fileData:
            logger.info("Received empty file data dictionary. No files to save.")
            return []

        for relativePath, content in fileData.items():
            # --- Final path validation just before writing ---
            # 1. Re-validate the relative path component itself (redundant if parseStructuredOutput is reliable, but safe)
            if not self._is_safe_relative_path(relativePath):
                # This indicates an internal issue if it wasn't caught during parsing.
                errMsg = f"Skipping file due to unsafe relative path discovered just before saving: '{relativePath}'"
                logger.critical(errMsg + ". This should have been caught during parsing.") # Log as critical
                raise FileProcessingError(errMsg) # Stop processing

            # 2. Construct the full path and perform critical safety check
            try:
                # Combine and normalize path using the *resolved* output directory
                # os.path.join is usually safe, but normpath cleans '..' etc.
                fullPath_normalized = os.path.normpath(os.path.join(resolvedOutputDir, relativePath))

                # Critical Check: Resolve the *candidate* full path and ensure it's still within the intended output directory.
                # This prevents issues if the relativePath contains tricky sequences like symlinks or components
                # that os.path.join/normpath might not fully mitigate in all edge cases across OSes.
                resolvedFullPath = os.path.realpath(fullPath_normalized)

                # Ensure the resolved path starts with the resolved output directory path.
                # Add os.sep to base path for reliable prefix checking (avoids matching /base/dir against /base/dir-other)
                base_check_path = os.path.join(resolvedOutputDir, '') # Ensures trailing separator

                if not resolvedFullPath.startswith(base_check_path):
                    context = f"resolves outside base directory ('{resolvedOutputDir}')"
                    errMsg = (f"Path validation failed for '{relativePath}'. "
                              f"Target '{resolvedFullPath}' {context}. Aborting save for safety.")
                    logger.critical(errMsg)
                    raise FileProcessingError(errMsg)

                # Also prevent writing directly *onto* the base directory itself (e.g., if relativePath was empty or '.')
                if resolvedFullPath == resolvedOutputDir:
                     errMsg = (f"Path validation failed for '{relativePath}'. "
                               f"Attempting to write directly onto the base directory '{resolvedOutputDir}'. Aborting save.")
                     logger.critical(errMsg)
                     raise FileProcessingError(errMsg)

            except OSError as e: # Catch errors during realpath resolution or checks
                errMsg = f"OS error resolving or checking final path for '{relativePath}' within '{resolvedOutputDir}': {e}"
                logger.error(errMsg)
                raise FileProcessingError(errMsg) from e
            except Exception as e: # Catch any other unexpected validation errors
                errMsg = f"Unexpected error validating final path for '{relativePath}': {e}"
                logger.error(errMsg, exc_info=True)
                raise FileProcessingError(errMsg) from e

            # --- Proceed to save ---
            logger.debug(f"Validated path. Preparing to save file: {resolvedFullPath}")
            try:
                # Get the directory part of the final path
                fileDir: str = os.path.dirname(resolvedFullPath)

                # Create necessary subdirectories only if needed (don't try to create the base dir itself)
                if fileDir != resolvedOutputDir and not os.path.isdir(fileDir):
                    os.makedirs(fileDir, exist_ok=True) # exist_ok=True prevents error if dir already exists (e.g., race condition)
                    logger.debug(f"Ensured directory exists: {fileDir}")

                # Write the file content (overwrite if exists) using UTF-8 encoding
                with open(resolvedFullPath, 'w', encoding='utf-8') as fileHandle:
                    fileHandle.write(content)

                savedFilesList.append(relativePath) # Append the original relative path
                logger.debug(f"Successfully wrote file: {resolvedFullPath} (relative: {relativePath})")

            except OSError as e:
                # Catch errors during makedirs or file writing (permissions, disk full, etc.)
                errMsg = f"OS error writing file '{resolvedFullPath}' (relative: '{relativePath}'): {e}"
                logger.error(errMsg, exc_info=True)
                # Stop on the first write error to prevent partial writes
                raise FileProcessingError(errMsg) from e
            except Exception as e:
                # Catch any other unexpected errors during file saving
                errMsg = f"An unexpected error occurred saving file '{resolvedFullPath}' (relative: '{relativePath}'): {e}"
                logger.error(errMsg, exc_info=True)
                # Stop on the first write error
                raise FileProcessingError(errMsg) from e

        logger.info(f"Successfully saved {len(savedFilesList)} files.")
        return savedFilesList

# --- END: core/file_processor.py ---